{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trabalho_rnn_FINAL_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WzYjYxpv5Hb",
        "outputId": "6d7f8b9c-4a67-45b3-949f-33b77a62144a"
      },
      "source": [
        "# Márcio Koch - Adaptado de: https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n",
        "# Pytorch MLP para classificação de multiplas classes\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from numpy import vstack\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Softmax\n",
        "from torch.nn import Module\n",
        "\n",
        "from torch.optim import SGD\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import NLLLoss\n",
        "from torch.nn import MSELoss\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch as T\n",
        "\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "labelEncoder = LabelEncoder()\n",
        "# Definição do dataset\n",
        "class CSVDataset(Dataset):    \n",
        "    # Carrega o data set\n",
        "    # path: caminho do arquivo CSV\n",
        "    # top: limite de linhas a serem carregadas do dataset, 0 = todas as linhas\n",
        "    def __init__(self, path, top):        \n",
        "        # Carrega o CSV como um dataframe, considera a primeira linha como linha de cabeçalho\n",
        "        df = read_csv(path, header=0, sep=';')\n",
        "\n",
        "        # Pega as top primeiras linhas do CSV\n",
        "        if (top > 0):\n",
        "          df = df.head(top)       \n",
        "\n",
        "        # Por conveniência, move a coluna com os labels para a última coluna do dataframe\n",
        "        y = df.pop('FORCE_2020_LITHOFACIES_LITHOLOGY')\n",
        "        df['FORCE_2020_LITHOFACIES_LITHOLOGY'] = y\n",
        "       \n",
        "        # Armazena as entradas e saídas\n",
        "        # X = entradas da rede\n",
        "        # y = saídas, classes que a rede prediz\n",
        "        self.X = df.values[:, :-1]\n",
        "        self.y = df.values[:, -1]\n",
        "      \n",
        "        # Garante que os dados de entrada são floats.\n",
        "        self.X = self.X.astype('float32')        \n",
        "       \n",
        "        # Codifica os labels no estilo binário, a classe predita será bit ligado (\"1\"), \n",
        "        # As demais classes terão bit desligado (\"0\").\n",
        "      \n",
        "        self.y = labelEncoder.fit_transform(self.y)        \n",
        "\n",
        "    # Retorna o número de linhas no dataset\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "   \n",
        "    # Obtém dados de treino e label de uma linha pelo seu índice\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X[idx], self.y[idx]]\n",
        "   \n",
        "    # Obtém índices para as linhas de treinamento e teste.\n",
        "    def get_splits(self, n_test=0.3):        \n",
        "        # Determina os tamanhos do conjunto de teste e treino\n",
        "        test_size = round(n_test * len(self.X))\n",
        "        train_size = len(self.X) - test_size\n",
        "\n",
        "        # Divide as amostras de dados a serem utilizadas\n",
        "        return random_split(self, [train_size, test_size])\n",
        "\n",
        "# Definição do modelo da rede neural\n",
        "class MLP(Module):\n",
        "\n",
        "    # Define os elementos do modelo\n",
        "    def __init__(self, n_inputs):\n",
        "      super(MLP, self).__init__()\n",
        "        \n",
        "      self.fc1 = nn.Linear(n_inputs, 200)\n",
        "      self.fc2 = nn.Linear(200, 100)\n",
        "      self.fc3 = nn.Linear(100, 80)\n",
        "      #self.fc4 = nn.Linear(80, 30)\n",
        "      self.fc5 = nn.Linear(80, 12)     \n",
        "\n",
        "      #20% probability Dropout \n",
        "      self.dropout = nn.Dropout(p=.2)\n",
        "   \n",
        "    # Entrada para o \"forward propagate\"\n",
        "    def forward(self, x):\n",
        "       x = self.dropout(F.relu(self.fc1(x)))\n",
        "       x = self.dropout(F.relu(self.fc2(x)))\n",
        "       x = self.dropout(F.relu(self.fc3(x)))\n",
        "       #x = self.dropout(F.relu(self.fc4(x)))\n",
        "       x = F.log_softmax(self.fc5(x), dim=1)\n",
        "       return x\n",
        "\n",
        "# Prepara o dataset\n",
        "def prepare_data(path, top):    \n",
        "    # Carrega o dataset\n",
        "    dataset = CSVDataset(path, top)\n",
        "    \n",
        "    # Calcula a divisão para treino e teste\n",
        "    train, test = dataset.get_splits()\n",
        "\n",
        "    # Prepara os carregadores de dados em batch\n",
        "    train_dl = DataLoader(train, batch_size=32, shuffle=True)    \n",
        "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
        "\n",
        "    return train_dl, test_dl\n",
        "\n",
        "# Avalia o modelo\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = list(), list()\n",
        "    for i, (inputs, targets) in enumerate(test_dl):        \n",
        "        # Avalia o modelo no conjunto de teste\n",
        "        yhat = model(inputs)\n",
        "        \n",
        "        # Retorna o array numpy\n",
        "        yhat = yhat.detach().numpy()        \n",
        "        actual = targets.numpy()\n",
        "       \n",
        "        # Converte para rótulos de classe\n",
        "        yhat = argmax(yhat, axis=1)\n",
        "        \n",
        "        # Remodela para empilhamento\n",
        "        actual = actual.reshape((len(actual), 1))\n",
        "        yhat = yhat.reshape((len(yhat), 1))\n",
        "       \n",
        "        # Armazena na lista de predições\n",
        "        predictions.append(yhat)\n",
        "        actuals.append(actual)\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "    \n",
        "    # Calcula a acurrácia\n",
        "    acc = accuracy_score(actuals, predictions)\n",
        "    return acc\n",
        "\n",
        "# Faz a predição da classe de uma data linha de dados desconhecidos\n",
        "def predict(row, model):    \n",
        "    # Converte o conteúdo da linha para dados\n",
        "    row = Tensor([row])      \n",
        "\n",
        "    # Faz a predição\n",
        "    yhat = model(row)   \n",
        "    \n",
        "    # Retorna o array numpy\n",
        "    yhat = yhat.detach().numpy()\n",
        "    return yhat\n",
        "\n",
        "# Faz as predições do dataset hidden e salva o resultado num arquivo CSV\n",
        "def predic_hidden(model, top):\n",
        "  path = '/content/drive/MyDrive/Furb/hidden.csv'\n",
        "  dfh = read_csv(path, header=0, sep=';')\n",
        "  \n",
        "  if (top == 0):\n",
        "    top = len(dfh)\n",
        "  hiddens = []\n",
        "\n",
        "  for i in range(0, top):   \n",
        "    row = dfh.iloc[i].values\n",
        "    yhat = predict(row, model)\n",
        "    yy = argmax(yhat)    \n",
        "\n",
        "    predictedLabel = str(int(labelEncoder.inverse_transform([yy])))    \n",
        "    hiddens.append(predictedLabel)   \n",
        "\n",
        "  np.savetxt(\"hidden_result.csv\", \n",
        "           hiddens,\n",
        "           delimiter =\",\", \n",
        "           fmt ='% s')\n",
        "  \n",
        "  print('DONE predic_hidden')\n",
        "  \n",
        "#############################################################################  \n",
        "# Treina o modelo\n",
        "def train_model(train_dl, model, epocas):\n",
        "    erros = 0    \n",
        "    # Define a estratégia de otimização\n",
        "    criterion = CrossEntropyLoss()    \n",
        "    \n",
        "    #optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    optimizer = Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "    # Enumera/itera pelas épocas    \n",
        "    for epoch in range(epocas):\n",
        "        print('Fazendo EPOCA:', epoch)\n",
        "        # Enumera/itera pelos mini batches\n",
        "        for i, (inputs, targets) in enumerate(train_dl):            \n",
        "            # Limpa os gradientes\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Computa o modelo de saída\n",
        "            yhat = model(inputs)\n",
        "            \n",
        "            # Calcula a perda (loss)\n",
        "            try:\n",
        "              loss = criterion(yhat, targets)\n",
        "             \n",
        "              # Faz o back propagation\n",
        "              loss.backward()\n",
        "             \n",
        "              # Atualiza os pesos do modelo\n",
        "              optimizer.step()\n",
        "            except  Exception as inst:\n",
        "              erros = erros + 1\n",
        "              print('Erro: ', inst);\n",
        "              print('targets: ', targets);\n",
        "\n",
        "    print('Total erros:', erros)\n",
        "#############################################################################      \n",
        "\n",
        "# Prepara os dados\n",
        "path = '/content/drive/MyDrive/Furb/lithology.csv'\n",
        "\n",
        "# Quantidade de linhas do dataset a serem utilizadas, 0 = todas\n",
        "top = 0\n",
        "\n",
        "# Quantidade de épocas de treinamento\n",
        "epocas = 100\n",
        "\n",
        "train_dl, test_dl = prepare_data(path, top)\n",
        "print(len(train_dl.dataset), len(test_dl.dataset))\n",
        "\n",
        "# Define a rede, quantidade de variáveis de entrada da rede\n",
        "model = MLP(30)\n",
        "\n",
        "# Treina o modelo\n",
        "train_model(train_dl, model, epocas)\n",
        "\n",
        "# Avalia o modelo\n",
        "acc = evaluate_model(test_dl, model)\n",
        "print('Accuracy: %.3f' % acc)\n",
        "\n",
        "print('DONE train')\n",
        "\n",
        "predic_hidden(model, 0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "819358 351153\n",
            "Fazendo EPOCA: 0\n",
            "Fazendo EPOCA: 1\n",
            "Fazendo EPOCA: 2\n",
            "Fazendo EPOCA: 3\n",
            "Fazendo EPOCA: 4\n",
            "Fazendo EPOCA: 5\n",
            "Fazendo EPOCA: 6\n",
            "Fazendo EPOCA: 7\n",
            "Fazendo EPOCA: 8\n",
            "Fazendo EPOCA: 9\n",
            "Fazendo EPOCA: 10\n",
            "Fazendo EPOCA: 11\n",
            "Fazendo EPOCA: 12\n",
            "Fazendo EPOCA: 13\n",
            "Fazendo EPOCA: 14\n",
            "Fazendo EPOCA: 15\n",
            "Fazendo EPOCA: 16\n",
            "Fazendo EPOCA: 17\n",
            "Fazendo EPOCA: 18\n",
            "Fazendo EPOCA: 19\n",
            "Fazendo EPOCA: 20\n",
            "Fazendo EPOCA: 21\n",
            "Fazendo EPOCA: 22\n",
            "Fazendo EPOCA: 23\n",
            "Fazendo EPOCA: 24\n",
            "Fazendo EPOCA: 25\n",
            "Fazendo EPOCA: 26\n",
            "Fazendo EPOCA: 27\n",
            "Fazendo EPOCA: 28\n",
            "Fazendo EPOCA: 29\n",
            "Fazendo EPOCA: 30\n",
            "Fazendo EPOCA: 31\n",
            "Fazendo EPOCA: 32\n",
            "Fazendo EPOCA: 33\n",
            "Fazendo EPOCA: 34\n",
            "Fazendo EPOCA: 35\n",
            "Fazendo EPOCA: 36\n",
            "Fazendo EPOCA: 37\n",
            "Fazendo EPOCA: 38\n",
            "Fazendo EPOCA: 39\n",
            "Fazendo EPOCA: 40\n",
            "Fazendo EPOCA: 41\n",
            "Fazendo EPOCA: 42\n",
            "Fazendo EPOCA: 43\n",
            "Fazendo EPOCA: 44\n",
            "Fazendo EPOCA: 45\n",
            "Fazendo EPOCA: 46\n",
            "Fazendo EPOCA: 47\n",
            "Fazendo EPOCA: 48\n",
            "Fazendo EPOCA: 49\n",
            "Fazendo EPOCA: 50\n",
            "Fazendo EPOCA: 51\n",
            "Fazendo EPOCA: 52\n",
            "Fazendo EPOCA: 53\n",
            "Fazendo EPOCA: 54\n",
            "Fazendo EPOCA: 55\n",
            "Fazendo EPOCA: 56\n",
            "Fazendo EPOCA: 57\n",
            "Fazendo EPOCA: 58\n",
            "Fazendo EPOCA: 59\n",
            "Fazendo EPOCA: 60\n",
            "Fazendo EPOCA: 61\n",
            "Fazendo EPOCA: 62\n",
            "Fazendo EPOCA: 63\n",
            "Fazendo EPOCA: 64\n",
            "Fazendo EPOCA: 65\n",
            "Fazendo EPOCA: 66\n",
            "Fazendo EPOCA: 67\n",
            "Fazendo EPOCA: 68\n",
            "Fazendo EPOCA: 69\n",
            "Fazendo EPOCA: 70\n",
            "Fazendo EPOCA: 71\n",
            "Fazendo EPOCA: 72\n",
            "Fazendo EPOCA: 73\n",
            "Fazendo EPOCA: 74\n",
            "Fazendo EPOCA: 75\n",
            "Fazendo EPOCA: 76\n",
            "Fazendo EPOCA: 77\n",
            "Fazendo EPOCA: 78\n",
            "Fazendo EPOCA: 79\n",
            "Fazendo EPOCA: 80\n",
            "Fazendo EPOCA: 81\n",
            "Fazendo EPOCA: 82\n",
            "Fazendo EPOCA: 83\n",
            "Fazendo EPOCA: 84\n",
            "Fazendo EPOCA: 85\n",
            "Fazendo EPOCA: 86\n",
            "Fazendo EPOCA: 87\n",
            "Fazendo EPOCA: 88\n",
            "Fazendo EPOCA: 89\n",
            "Fazendo EPOCA: 90\n",
            "Fazendo EPOCA: 91\n",
            "Fazendo EPOCA: 92\n",
            "Fazendo EPOCA: 93\n",
            "Fazendo EPOCA: 94\n",
            "Fazendo EPOCA: 95\n",
            "Fazendo EPOCA: 96\n",
            "Fazendo EPOCA: 97\n",
            "Fazendo EPOCA: 98\n",
            "Fazendo EPOCA: 99\n",
            "Total erros: 0\n",
            "Accuracy: 0.616\n",
            "DONE train\n",
            "DONE predic_hidden\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}